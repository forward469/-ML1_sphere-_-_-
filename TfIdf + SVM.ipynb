{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from multiprocessing import Pool, Lock, Value\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение всех заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "\n",
    "import re\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def to_digit(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "def remove_urls (vTEXT):\n",
    "    regex = re.compile(\n",
    "    # r'^(?:http|ftp)s?://'  # http:// or https://\n",
    "    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  # domain...\n",
    "    r'localhost|'  # localhost...\n",
    "    r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|'  # ...or ipv4\n",
    "    r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)'  # ...or ipv6\n",
    "    r'(?::\\d+)?'  # optional port\n",
    "    r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    \n",
    "    vTEXT = re.sub(regex, '', vTEXT)\n",
    "    return(vTEXT)\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove urls\n",
    "        # document = re.sub(url_reg, '', document)\n",
    "        document = remove_urls(document)\n",
    "        \n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[а-яА-Я]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[а-яА-Я]\\s+', ' ', str(document))\n",
    "        \n",
    "        # Remove 1-2 digits\n",
    "#         document = re.sub(r'\\b[0-9]\\b', ' ', str(document))\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', str(document), flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        # print(tokens)\n",
    "        # tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        # tokens = [word for word in tokens if word not in en_stop]\n",
    "        tokens = [word for word in tokens \\\n",
    "                  if (len(word) > 2)|\\\n",
    "                  (type(to_digit(word)) == int)|\\\n",
    "                  (word.lower() in ['cs','go','vc'])]\n",
    "        # print(tokens)\n",
    "\n",
    "        preprocessed_text = ' '.join(list(map(lambda x: morph.parse(x)[0].normal_form, tokens )))\n",
    "\n",
    "        return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001887388911417481d75f197f05b62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28026), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a267a5566c648cdaf9e2d4729cc213e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27994), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Only titles\n",
    "docs_titles = list(doc_to_title.values())\n",
    "final_corpus = [preprocess_text(sentence) for sentence in tqdm(docs_titles) if sentence.strip() !='']\n",
    "\n",
    "word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
    "word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in tqdm(final_corpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l1', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vect = TfidfVectorizer(norm='l1')\n",
    "Vect.fit(final_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "from scipy.spatial.distance import cosine, cdist\n",
    "\n",
    "def get_embedding(vec, group_Matrix, thrshold = 0.03):\n",
    "    # return sorted((vec * group_Matrix > thrshold).sum(axis=1), reverse=True)[1:25]\n",
    "    return sorted((vec * group_Matrix).sum(axis=1), reverse=True)[1:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4016a2b61ab747d19f5babfd2af84358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11690), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = preprocess_text(doc_to_title[doc_id])\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb6452c766642308c6d2b971d25f8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=129), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(11690, 24) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in tqdm(traingroups_titledata):\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    group_TfIdf_Matrix = Vect.transform([title for _, title, _ in docs]).toarray()\n",
    "    for k, ((doc_id, title, target_id), vec) in enumerate(zip(docs, group_TfIdf_Matrix)):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        X_train.append(get_embedding(vec, group_TfIdf_Matrix))\n",
    "\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros((X_train.shape[0], max(map(len, X_train))))\n",
    "for i, row in enumerate(X_train):\n",
    "    for j, val in enumerate(row):\n",
    "        arr[i,j]=val\n",
    "X_train = np.array(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04f3a43fd324f6faf8d62d0c36b8172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16627), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('test_groups.csv')\n",
    "testgroups_titledata = {}\n",
    "for i in tqdm(range(test_data.shape[0])):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    pair_id = new_doc['pair_id']\n",
    "    title = preprocess_text(doc_to_title[doc_id])\n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "    testgroups_titledata[doc_group].append((doc_id, pair_id, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5bce2ac3a1423a9bec39070ed2887f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(16627, 24)\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "pairs_id = []\n",
    "for new_group in tqdm(testgroups_titledata):\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    group_TfIdf_Matrix = Vect.transform([title for _, _, title in docs]).toarray()\n",
    "    for k, ((doc_id, pair_id, title), vec) in enumerate(zip(docs, group_TfIdf_Matrix)):\n",
    "        X_test.append(get_embedding(vec, group_TfIdf_Matrix))\n",
    "        pairs_id.append(pair_id)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros((X_test.shape[0], max(map(len, X_test))))\n",
    "for i, row in enumerate(X_test):\n",
    "    for j, val in enumerate(row):\n",
    "        arr[i,j]=val\n",
    "X_test = np.array(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest, product\n",
    "\n",
    "prev_group = 1\n",
    "prev_index = 0\n",
    "groups_indices = []\n",
    "for k, i in zip_longest(range(len(groups_train) + 1),\n",
    "                        groups_train, fillvalue=-1):\n",
    "    if prev_group != i:\n",
    "        groups_indices.append([prev_index, k])\n",
    "        prev_group = i\n",
    "        prev_index = k\n",
    "groups_indices = np.array(groups_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(clf, trsh, X):\n",
    "    proba = clf.predict_proba(X)\n",
    "    return np.array(list(map(lambda x: 1 if x[1] > trsh else 0  , proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b205bd75b2b7442c806aafa90ca0c8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d0914f6f820d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrsh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVal_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/sphere-py37/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/sphere-py37/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PENALTY = ['l2']\n",
    "LOSS = ['squared_hinge']\n",
    "THRSHS = np.arange(0.05, 1.0, 0.05)\n",
    "REGUL_C = np.logspace(-2,1,4) #np.arange(1,241,40) #np.arange(1,201,40)\n",
    "\n",
    "Params = list(product(REGUL_C, LOSS, PENALTY, THRSHS))\n",
    "\n",
    "result = np.zeros(len(Params))\n",
    "\n",
    "for i, (C, loss, penalty, trsh) in tqdm(list(enumerate(Params))):\n",
    "    f_scores = []\n",
    "    roc_aucs = []\n",
    "    kf = KFold(n_splits=6)\n",
    "    for train_index, val_index in kf.split(groups_indices):\n",
    "        Train_Indicies = reduce(lambda x,y: x+y, map(lambda x: list(range(x[0],x[1])), groups_indices[train_index]))\n",
    "        Val_Indicies = reduce(lambda x,y: x+y, map(lambda x: list(range(x[0],x[1])), groups_indices[val_index]))\n",
    "\n",
    "        Train_X, Train_y = X_train[Train_Indicies], y_train[Train_Indicies]\n",
    "        Val_X, Val_y = X_train[Val_Indicies], y_train[Val_Indicies]\n",
    "\n",
    "        clf = SVC(C=C, probability=True)\n",
    "        clf.fit(Train_X, Train_y)\n",
    "        preds = predict(clf, trsh, Val_X)\n",
    "        \n",
    "        f_scores.append(f1_score(Val_y, preds))\n",
    "#         roc_aucs.append(roc_auc_score(Val_y, preds))\n",
    "        \n",
    "        result[i] = np.mean(f_scores)\n",
    "#         result[i] = np.mean(roc_aucs)\n",
    "        \n",
    "best_index = result.argmax()\n",
    "BEST_LOSS = Params[best_index][1]\n",
    "BEST_REGUL_C = Params[best_index][0]\n",
    "BEST_PENALTY = Params[best_index][2]\n",
    "BEST_T = Params[best_index][3]\n",
    "print(BEST_REGUL_C, BEST_LOSS, BEST_PENALTY, result[best_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores = []\n",
    "roc_aucs = []\n",
    "kf = KFold(n_splits=6)\n",
    "for train_index, val_index in kf.split(groups_indices):\n",
    "    Train_Indicies = reduce(lambda x,y: x+y, map(lambda x: list(range(x[0],x[1])), groups_indices[train_index]))\n",
    "    Val_Indicies = reduce(lambda x,y: x+y, map(lambda x: list(range(x[0],x[1])), groups_indices[val_index]))\n",
    "\n",
    "    Train_X, Train_y = X_train[Train_Indicies], y_train[Train_Indicies]\n",
    "    Val_X, Val_y = X_train[Val_Indicies], y_train[Val_Indicies]\n",
    "\n",
    "    clf = SVC(C=0.01, probability=True)\n",
    "    clf.fit(Train_X, Train_y)\n",
    "    preds = predict(clf, 0.3, Val_X)\n",
    "\n",
    "    f_scores.append(f1_score(Val_y, preds))\n",
    "#         roc_aucs.append(roc_auc_score(Val_y, preds))\n",
    "\n",
    "    result[i] = np.mean(f_scores)\n",
    "#         result[i] = np.mean(roc_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6940817107109333"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Preds = []\n",
    "f_scores = []\n",
    "roc_aucs = []\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "\n",
    "for train_index, val_index in kf.split(groups_indices):\n",
    "    Train_Indicies = reduce(lambda x,y: x+y, map(lambda x: list(range(x[0],x[1])), groups_indices[train_index]))\n",
    "    Val_Indicies = reduce(lambda x,y: x+y, map(lambda x: list(range(x[0],x[1])), groups_indices[val_index]))\n",
    "\n",
    "    Train_X, Train_y = X_train[Train_Indicies], y_train[Train_Indicies]\n",
    "    Val_X, Val_y = X_train[Val_Indicies], y_train[Val_Indicies]\n",
    "\n",
    "    clf = SVC(C=100, probability=True)\n",
    "    clf.fit(Train_X, Train_y)\n",
    "    preds = predict(clf, 0.25, Val_X)\n",
    "    Test_Preds.append(predict(clf, 0.25, X_test))\n",
    "    f_scores.append(f1_score(Val_y, preds))\n",
    "    roc_aucs.append(roc_auc_score(Val_y, preds))\n",
    "Test_Preds = np.array(Test_Preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f-score`s :  [0.658 0.686 0.685 0.765] 0.69850008184307\n",
      "ROC-AUC score`s :  [0.795 0.785 0.816 0.798] 0.7985480700938516\n"
     ]
    }
   ],
   "source": [
    "print('f-score`s : ', np.round(f_scores,3), np.mean(f_scores))\n",
    "print('ROC-AUC score`s : ', np.round(roc_aucs,3), np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
